{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:10.106702Z",
     "start_time": "2020-06-14T02:52:08.026148Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:24.104165Z",
     "start_time": "2020-06-14T02:52:24.094795Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def json_read(filename):\n",
    "    with open(filename, 'r') as inf:\n",
    "        res = json.load(inf)\n",
    "    return res\n",
    "\n",
    "def json_dump(obj, filename, ea=False, indent=4):\n",
    "    with open(filename, 'w') as ouf:\n",
    "        json.dump(obj, ouf, ensure_ascii=ea, indent=indent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Elasticsearch\n",
    "\n",
    "Note you must have an Elasticsearch index of Wikidata entities. To create the index you may use [this] notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:28.367409Z",
     "start_time": "2020-06-14T02:52:28.359660Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load linker classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:30.172537Z",
     "start_time": "2020-06-14T02:52:30.163354Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Morpher:\n",
    "    def __init__(self):\n",
    "        self.stop_pos_tags = set([\n",
    "            'ADVPRO',\n",
    "            'APRO',\n",
    "            'CONJ',\n",
    "            'INTJ',\n",
    "            'PART',\n",
    "            'PR',\n",
    "            'SPRO',\n",
    "            'V',\n",
    "            'ADV'\n",
    "        ])\n",
    "        self.mystem = Mystem()\n",
    "        self.tokenizer = CountVectorizer(lowercase=False, token_pattern='\\w+').build_analyzer()\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        return(' '.join(self.tokenizer(text)))\n",
    "        \n",
    "    def analyze(self, text):\n",
    "        return self.mystem.analyze(text)\n",
    "    \n",
    "    def approve_tag(self, tag):\n",
    "        if tag in self.stop_pos_tags:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:30.705363Z",
     "start_time": "2020-06-14T02:52:30.671691Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, text, morpher):\n",
    "        self.text = text\n",
    "        self.morpher = morpher\n",
    "        self.save_analysis()\n",
    "        self.save_filtered_tokens()\n",
    "        self.save_token_ngrams()\n",
    "        self.save_capital_pairs()\n",
    "        self.save_capital_singles()\n",
    "        \n",
    "    def save_analysis(self):\n",
    "        text = self.morpher.preprocess(self.text)\n",
    "        analysis = self.morpher.analyze(text)\n",
    "        self.analysis = []\n",
    "        for entry in analysis:\n",
    "            if not re.fullmatch(r'\\s*', entry['text']):\n",
    "                self.analysis.append(entry)\n",
    "                \n",
    "    def _get_lemma_from_analysis(self, entry):\n",
    "        if 'analysis' not in entry or \\\n",
    "           not entry['analysis'] or \\\n",
    "           'lex' not in entry['analysis'][0]:\n",
    "            return entry['text']\n",
    "        return entry['analysis'][0]['lex']\n",
    "                \n",
    "    def save_filtered_tokens(self):\n",
    "        self.filtered_tokens = []\n",
    "        for entry in self.analysis:\n",
    "            if 'analysis' in entry:\n",
    "                if not entry['analysis']:\n",
    "                    self.filtered_tokens.append(self._get_lemma_from_analysis(entry))\n",
    "                    continue\n",
    "                if entry['text'][0].isupper() and len(entry['text']) > 1:\n",
    "                    self.filtered_tokens.append(self._get_lemma_from_analysis(entry))\n",
    "                    continue\n",
    "                pos_tag = entry['analysis'][0]['gr'].split(',', 1)[0].split('=', 1)[0]\n",
    "                if self.morpher.approve_tag(pos_tag):\n",
    "                    self.filtered_tokens.append(self._get_lemma_from_analysis(entry))\n",
    "    \n",
    "    def save_token_ngrams(self, n=3):\n",
    "        self.token_ngrams = []\n",
    "        for i in range(len(self.analysis) - (n - 1)):\n",
    "            self.token_ngrams.append(' '.join([self._get_lemma_from_analysis(entry) for entry in self.analysis[i:(i + n)]]))\n",
    "        return self.token_ngrams\n",
    "    \n",
    "    def save_capital_pairs(self):\n",
    "        self.capital_pairs = []\n",
    "        for i in range(0, len(self.analysis) - 1):\n",
    "            if self.analysis[i]['text'][0].isupper() and \\\n",
    "               len(self.analysis[i]['text']) > 1 and \\\n",
    "               self.analysis[i + 1]['text'][0].isupper() and \\\n",
    "               len(self.analysis[i + 1]['text']) > 1:\n",
    "                self.capital_pairs.append(' '.join([self._get_lemma_from_analysis(entry) for entry in self.analysis[i:(i + 2)]]))\n",
    "        return self.capital_pairs\n",
    "    \n",
    "    def save_capital_singles(self):\n",
    "        self.capital_singles = []\n",
    "        for entry in self.analysis:\n",
    "            if entry['text'][0].isupper() and \\\n",
    "               len(entry['text']) > 2:\n",
    "                self.capital_singles.append(self._get_lemma_from_analysis(entry))\n",
    "        return self.capital_singles\n",
    "    \n",
    "    def build_match_query(self, query, fuzziness='AUTO'):\n",
    "        return  { \n",
    "                    'match': {\n",
    "                        'label': {\n",
    "                            'query': query,\n",
    "                            \"fuzziness\": fuzziness,\n",
    "                            \"prefix_length\": 1,\n",
    "                            'fuzzy_transpositions': False\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    def build_phrase_query(self, query):\n",
    "        return  { \n",
    "                    'match_phrase': {\n",
    "                        'label': query\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    def get_phrase_queries(self):\n",
    "        qs = []\n",
    "        for ng in self.token_ngrams:\n",
    "            qs.append(self.build_phrase_query(ng))\n",
    "        for cp in self.capital_pairs:\n",
    "            qs.append(self.build_phrase_query(cp))\n",
    "        return qs\n",
    "    \n",
    "    def get_fulltext_queries(self):\n",
    "        return [self.build_match_query(' '.join(self.filtered_tokens))]\n",
    "    \n",
    "    def get_single_capital_queries(self):\n",
    "        qs = []\n",
    "        for cs in self.capital_singles:\n",
    "            qs.append(self.build_phrase_query(cs))\n",
    "        return qs\n",
    "        \n",
    "    def build_es_query(self, queries):\n",
    "        return  {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'should': queries\n",
    "                        }\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:31.280142Z",
     "start_time": "2020-06-14T02:52:31.265027Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class QueryQuestion(Query):\n",
    "    def save_filtered_tokens(self):\n",
    "        self.filtered_tokens = []\n",
    "        for entry in self.analysis[1:]:\n",
    "            if 'analysis' in entry:\n",
    "                if not entry['analysis']:\n",
    "                    self.filtered_tokens.append(self._get_lemma_from_analysis(entry))\n",
    "                    continue\n",
    "                if entry['text'][0].isupper() and len(entry['text']) > 1:\n",
    "                    self.filtered_tokens.append(self._get_lemma_from_analysis(entry))\n",
    "                    continue\n",
    "                pos_tag = entry['analysis'][0]['gr'].split(',', 1)[0].split('=', 1)[0]\n",
    "                if self.morpher.approve_tag(pos_tag):\n",
    "                    self.filtered_tokens.append(self._get_lemma_from_analysis(entry))\n",
    "                    \n",
    "    def save_capital_pairs(self):\n",
    "        self.capital_pairs = []\n",
    "        for i in range(1, len(self.analysis) - 1):\n",
    "            if self.analysis[i]['text'][0].isupper() and \\\n",
    "               len(self.analysis[i]['text']) > 1 and \\\n",
    "               self.analysis[i + 1]['text'][0].isupper() and \\\n",
    "               len(self.analysis[i + 1]['text']) > 1:\n",
    "                self.capital_pairs.append(' '.join([self._get_lemma_from_analysis(entry) for entry in self.analysis[i:(i + 2)]]))\n",
    "        return self.capital_pairs\n",
    "    \n",
    "    def save_capital_singles(self):\n",
    "        self.capital_singles = []\n",
    "        for entry in self.analysis[1:]:\n",
    "            if entry['text'][0].isupper() and \\\n",
    "               len(entry['text']) > 2:\n",
    "                self.capital_singles.append(self._get_lemma_from_analysis(entry))\n",
    "        return self.capital_singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:32.003346Z",
     "start_time": "2020-06-14T02:52:31.993790Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class QueryAnswer(Query):\n",
    "    def save_token_ngrams(self, n=2):\n",
    "        self.token_ngrams = [' '.join([self._get_lemma_from_analysis(entry) for entry in self.analysis])]\n",
    "        for i in range(len(self.analysis) - (n - 1)):\n",
    "            self.token_ngrams.append(' '.join([self._get_lemma_from_analysis(entry) for entry in self.analysis[i:(i + n)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:32.846253Z",
     "start_time": "2020-06-14T02:52:32.818982Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Matcher:\n",
    "    def __init__(self, es_instance):\n",
    "        self.es = es_instance\n",
    "        self.wiki_links = json_read('wiki_links.json')\n",
    "        self.pageviews = json_read('pageviews_by_entity.json')\n",
    "    \n",
    "    def get_names_and_descriptions(self, qids):\n",
    "        if not qids:\n",
    "            return {}\n",
    "        qids_list = '|'.join(qids)\n",
    "        wikiapi_query = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={qids_list}&languages=ru&props=labels|descriptions'\n",
    "        resp = requests.get(wikiapi_query).json()\n",
    "        result = {}\n",
    "        for qid in qids:\n",
    "            cur_data = resp['entities'][qid]\n",
    "            name = None\n",
    "            if 'labels' in cur_data:\n",
    "                if 'ru' in cur_data['labels']:\n",
    "                    name = cur_data['labels']['ru']['value']\n",
    "            description = None\n",
    "            if 'descriptions' in cur_data:\n",
    "                if 'ru' in cur_data['descriptions']:\n",
    "                    description = cur_data['descriptions']['ru']['value']\n",
    "            result[qid] = {'name': name, 'description': description}\n",
    "        return result  \n",
    "\n",
    "#     Fastest API\n",
    "#     def get_wikipedia_pageviews(self, titles_dict):\n",
    "#         if not titles_dict:\n",
    "#             return\n",
    "#         titles_list = list(map(lambda s: s.replace(' ', '_'), titles_dict.keys()))\n",
    "#         for i in range(len(titles_list)):\n",
    "#             titles = '|'.join(titles_list)\n",
    "#             wikiapi_query = f'https://ru.wikipedia.org/w/api.php?action=query&format=json&prop=pageviews&pvipdays=30&titles={titles}'\n",
    "#             resp = requests.get(wikiapi_query).json()\n",
    "#             if 'batchcomplete' in resp:\n",
    "#                 break\n",
    "#         pages_data = (resp['query']['pages'])\n",
    "        \n",
    "#         for entry in pages_data.values():\n",
    "#             view_stats = entry['pageviews']\n",
    "#             views = sum(filter(None, view_stats.values()))\n",
    "#             cur_title = entry['title']\n",
    "#             titles_dict[cur_title] = views\n",
    "            \n",
    "#     Slower API\n",
    "#     def get_wikipedia_pageviews(self, wiki_title):\n",
    "#         wiki_title = wiki_title.replace(' ', '_')\n",
    "#         wikiapi_query = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ru.wikipedia/all-access/all-agents/{wiki_title}/monthly/2019010100/2019013100'\n",
    "#         resp = requests.get(wikiapi_query).json()\n",
    "#         return resp['items'][0]['views']\n",
    "\n",
    "    def get_wikipedia_pages(self, qids):\n",
    "        result = {qid: {} for qid in qids}\n",
    "        for qid in qids:\n",
    "            if qid in self.wiki_links:\n",
    "                result[qid]['ruwiki'] = self.wiki_links[qid]\n",
    "            else:\n",
    "                result[qid]['ruwiki'] = None\n",
    "            if qid in self.pageviews:\n",
    "                result[qid]['views'] = self.pageviews[qid]\n",
    "            else:\n",
    "                result[qid]['views'] = 0\n",
    "        return result\n",
    "    \n",
    "#     Using Wikidata API\n",
    "#     def get_wikipedia_pages(self, qids):\n",
    "#         if not qids:\n",
    "#             return {}\n",
    "#         qids_list = '|'.join(qids)\n",
    "#         wikiapi_query = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={qids_list}&sitefilter=ruwiki&props=sitelinks'\n",
    "#         resp = requests.get(wikiapi_query).json()\n",
    "#         result = {}\n",
    "#         views = {}\n",
    "#         for qid in qids:\n",
    "#             cur_data = resp['entities'][qid]\n",
    "#             wiki_title = None\n",
    "#             if 'sitelinks' in cur_data and 'ruwiki' in cur_data['sitelinks'] and 'title' in cur_data['sitelinks']['ruwiki']:\n",
    "#                 wiki_title = cur_data['sitelinks']['ruwiki']['title']\n",
    "#                 views[wiki_title] = 0\n",
    "#             result[qid] = {'ruwiki': wiki_title}\n",
    "            \n",
    "#         self.get_wikipedia_pageviews(views)\n",
    "#         for qid in qids:\n",
    "#             cur_title = result[qid]['ruwiki']\n",
    "#             if cur_title is not None:\n",
    "#                 result[qid]['views'] = views[cur_title]\n",
    "#             else:\n",
    "#                 result[qid]['views'] = 0\n",
    "#         return result\n",
    "\n",
    "    def _apply_ranking(self, matches, relative_rate=0.9):\n",
    "        if len(matches) == 0:\n",
    "            return matches\n",
    "        \n",
    "        matches = sorted(matches, key=lambda m: m['score'], reverse=True)      \n",
    "        \n",
    "        cur_max_pos = 0\n",
    "        cur_pos = 1\n",
    "        while cur_pos < len(matches):\n",
    "            cur_max_score = matches[cur_max_pos]['score']\n",
    "            while cur_pos < len(matches) and matches[cur_pos]['score'] >= relative_rate * matches[cur_max_pos]['score']:\n",
    "                cur_pos += 1\n",
    "            matches[cur_max_pos:cur_pos] = sorted(matches[cur_max_pos:cur_pos], key=lambda m: m['views'], reverse=True)\n",
    "            cur_max_pos = cur_pos\n",
    "            cur_pos = cur_max_pos + 1\n",
    "            \n",
    "        return matches\n",
    "        \n",
    "    def get_query_matches(self, query, n_matches=30):\n",
    "        # run query and collect results\n",
    "        es_result = es.search(index='all_entities', body=query, size=n_matches)['hits']\n",
    "        matches_dict = {}\n",
    "        for entry in es_result['hits']:\n",
    "            qid = entry['_source']['qid']\n",
    "            if qid not in matches_dict:\n",
    "                matches_dict[qid] = entry['_source']\n",
    "                matches_dict[qid]['score'] = entry['_score']\n",
    "                \n",
    "        # obtain label and description for each qid\n",
    "        names_and_descriptions = self.get_names_and_descriptions(matches_dict.keys())\n",
    "        for qid, nds in names_and_descriptions.items():\n",
    "            matches_dict[qid].update(nds)\n",
    "            \n",
    "        # obtain wikipedia page and popularity for each qid\n",
    "        wikipedia_pages = self.get_wikipedia_pages(matches_dict.keys())\n",
    "        for qid, wps in wikipedia_pages.items():\n",
    "            matches_dict[qid].update(wps)\n",
    "            \n",
    "        # rank matched entities\n",
    "        matches = self._apply_ranking(list(matches_dict.values()))\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:33.491879Z",
     "start_time": "2020-06-14T02:52:33.469987Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Suggester:\n",
    "    def __init__(self, matcher, morpher):\n",
    "        self.matcher = matcher\n",
    "        self.morpher = morpher\n",
    "        \n",
    "    def _build_query(self, text):\n",
    "        return Query(text=text, morpher=self.morpher)\n",
    "    \n",
    "    def _get_matches(self, q, query_type):\n",
    "        if query_type == 'fulltext':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_fulltext_queries()))\n",
    "        if query_type == 'phrase':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_phrase_queries()))\n",
    "        if query_type == 'single':\n",
    "            single_queries = q.get_single_capital_queries()\n",
    "            return [self.matcher.get_query_matches(q.build_es_query(single_query))\n",
    "                   for single_query in single_queries]\n",
    "        \n",
    "    def _select_matches(self, q, min_total, f, p, s):\n",
    "        matches = []\n",
    "        matches_qids = set()\n",
    "        \n",
    "        phrase_matches = self._get_matches(q, query_type='phrase')\n",
    "        if len(phrase_matches) > p:\n",
    "            phrase_matches = phrase_matches[:p]\n",
    "        for match in phrase_matches:\n",
    "            matches_qids.add(match['qid'])\n",
    "            match['source'] = 'phrase'\n",
    "        \n",
    "        single_matches = self._get_matches(q, query_type='single')\n",
    "        single_matches_result = []\n",
    "        for single_match in single_matches:\n",
    "            if not single_match:\n",
    "                continue\n",
    "            top_match = single_match[0]\n",
    "            if top_match['qid'] not in matches_qids:\n",
    "                matches_qids.add(top_match['qid'])\n",
    "                top_match['source'] = 'single'\n",
    "                single_matches_result.append(top_match)\n",
    "        single_matches_result = sorted(single_matches_result, key=lambda m: m['views'], reverse=True)\n",
    "        \n",
    "        fulltext_matches = self._get_matches(q, query_type='fulltext')\n",
    "        fulltext_matches_result = []\n",
    "        f_cnt = 0\n",
    "        for match in fulltext_matches:\n",
    "            if match['qid'] not in matches_qids:\n",
    "                f_cnt += 1\n",
    "                matches_qids.add(match['qid'])\n",
    "                match['source'] = 'fulltext'\n",
    "                fulltext_matches_result.append(match)\n",
    "            if len(matches_qids) >= min_total and f_cnt >= f:\n",
    "                break\n",
    "        fulltext_matches_result = sorted(fulltext_matches_result, key=lambda m: m['score'], reverse=True)\n",
    "                \n",
    "        matches.extend(phrase_matches)\n",
    "        matches.extend(single_matches_result)\n",
    "        matches.extend(fulltext_matches_result)\n",
    "        return matches\n",
    "    \n",
    "    def get_suggestions(self, text, answer=''):\n",
    "        q = self._build_query(text)\n",
    "\n",
    "        matches = self._select_matches(q)\n",
    "        return {\n",
    "            'text': text,\n",
    "            'answer': answer,\n",
    "            'matches': matches\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def pretty_print(entry):\n",
    "        lines = []\n",
    "        \n",
    "        matches = entry['matches']\n",
    "        query = entry['text']\n",
    "        \n",
    "        lines.append(f'Query: {query}\\n')\n",
    "        \n",
    "        for match in matches:\n",
    "            if match['name']:\n",
    "                lines.append(match['name'])\n",
    "            else:\n",
    "                lines.append('*no name*')\n",
    "\n",
    "            if match['description']:\n",
    "                desc = match['description']\n",
    "                lines.append(f'({desc})')\n",
    "\n",
    "            qid = match['qid']\n",
    "            lines.append(f'Link: https://www.wikidata.org/wiki/{qid}\\n')\n",
    "        \n",
    "        return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:34.224015Z",
     "start_time": "2020-06-14T02:52:34.214609Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class SuggesterQuestion(Suggester):\n",
    "    def _build_query(self, text):\n",
    "        return QueryQuestion(text=text, morpher=self.morpher)\n",
    "    \n",
    "    def _get_matches(self, q, query_type):\n",
    "        if query_type == 'fulltext':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_fulltext_queries()), n_matches=20)\n",
    "        if query_type == 'phrase':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_phrase_queries()), n_matches=8)\n",
    "        if query_type == 'single':\n",
    "            single_queries = q.get_single_capital_queries()\n",
    "            return [self.matcher.get_query_matches(q.build_es_query(single_query), n_matches=10)\n",
    "                   for single_query in single_queries]\n",
    "        \n",
    "    def _select_matches(self, q, min_total=8, f=5, p=4, s=1):\n",
    "        return super()._select_matches(q, min_total, f, p, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:34.738529Z",
     "start_time": "2020-06-14T02:52:34.727404Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class SuggesterAnswer(Suggester):\n",
    "    def _build_query(self, text):\n",
    "        return QueryAnswer(text=text, morpher=self.morpher)\n",
    "    \n",
    "    def _get_matches(self, q, query_type):\n",
    "        if query_type == 'fulltext':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_fulltext_queries()), n_matches=5)\n",
    "        if query_type == 'phrase':\n",
    "            return self.matcher.get_query_matches(q.build_es_query(q.get_phrase_queries()), n_matches=10)\n",
    "        if query_type == 'single':\n",
    "            single_queries = q.get_single_capital_queries()\n",
    "            return [self.matcher.get_query_matches(q.build_es_query(single_query), n_matches=5)\n",
    "                   for single_query in single_queries]\n",
    "    \n",
    "    def _select_matches(self, q, min_total=5, f=3, p=4, s=1):\n",
    "        return super()._select_matches(q, min_total, f, p, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to prepare linkers for questions and answers is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:52:43.312061Z",
     "start_time": "2020-06-14T02:52:37.080227Z"
    }
   },
   "outputs": [],
   "source": [
    "mrph = Morpher()\n",
    "mtc = Matcher(es_instance=es)\n",
    "suggester_question = SuggesterQuestion(matcher=mtc, morpher=mrph)\n",
    "suggester_answer = SuggesterAnswer(matcher=mtc, morpher=mrph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right then you can get list of the candidate entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:57:29.718745Z",
     "start_time": "2020-06-14T02:57:25.501288Z"
    }
   },
   "outputs": [],
   "source": [
    "suggestions_question = suggester_question.get_suggestions('В каком американском штате находится Большой Каньон?')\n",
    "suggestions_answer = suggester_answer.get_suggestions('Аризона')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting suggestions are in JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:58:39.653974Z",
     "start_time": "2020-06-14T02:58:39.644082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'В каком американском штате находится Большой Каньон?',\n",
       " 'answer': '',\n",
       " 'matches': [{'qid': 'Q118841',\n",
       "   'label': 'большой каньон',\n",
       "   'score': 17.875689,\n",
       "   'name': 'Большой каньон',\n",
       "   'description': 'каньон, прорезанный рекой Колорадо в плато Колорадо, штат Аризона, США',\n",
       "   'ruwiki': 'Большой_каньон',\n",
       "   'views': 4756,\n",
       "   'source': 'phrase'},\n",
       "  {'qid': 'Q3848004',\n",
       "   'label': 'большой каньон',\n",
       "   'score': 17.875689,\n",
       "   'name': 'Большой каньон',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Большой_каньон_(Крым)',\n",
       "   'views': 391,\n",
       "   'source': 'phrase'},\n",
       "  {'qid': 'Q1542548',\n",
       "   'label': 'большой каньон',\n",
       "   'score': 17.875689,\n",
       "   'name': 'Большой каньон',\n",
       "   'description': 'фильм 1991 года',\n",
       "   'ruwiki': 'Большой_каньон_(фильм)',\n",
       "   'views': 173,\n",
       "   'source': 'phrase'},\n",
       "  {'qid': 'Q32508401',\n",
       "   'label': 'большой каньон',\n",
       "   'score': 17.875689,\n",
       "   'name': 'Большой Каньон',\n",
       "   'description': None,\n",
       "   'ruwiki': None,\n",
       "   'views': 0,\n",
       "   'source': 'phrase'},\n",
       "  {'qid': 'Q11442',\n",
       "   'label': 'большой',\n",
       "   'score': 8.274445,\n",
       "   'name': 'велосипед',\n",
       "   'description': 'двухколёсное транспортное средство, приводимое в движение посредством педалей',\n",
       "   'ruwiki': 'Велосипед',\n",
       "   'views': 6854,\n",
       "   'source': 'single'},\n",
       "  {'qid': 'Q150784',\n",
       "   'label': 'каньон',\n",
       "   'score': 14.028197,\n",
       "   'name': 'каньон',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Каньон',\n",
       "   'views': 1401,\n",
       "   'source': 'single'},\n",
       "  {'qid': 'Q16627582',\n",
       "   'label': 'большой каньон крым',\n",
       "   'score': 13.396823,\n",
       "   'name': 'Большой каньон Крыма',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Большой_каньон_Крыма_(заказник)',\n",
       "   'views': 37,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q31626940',\n",
       "   'label': 'гора большой каньон',\n",
       "   'score': 13.396823,\n",
       "   'name': 'Гора Большой Каньон',\n",
       "   'description': None,\n",
       "   'ruwiki': None,\n",
       "   'views': 0,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q30',\n",
       "   'label': 'северо американский соединять штат',\n",
       "   'score': 13.096247,\n",
       "   'name': 'США',\n",
       "   'description': 'федеративное государство в Северной Америке',\n",
       "   'ruwiki': 'Соединённые_Штаты_Америки',\n",
       "   'views': 90699,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q3014145',\n",
       "   'label': 'каньон',\n",
       "   'score': 11.757884,\n",
       "   'name': 'Каньоны',\n",
       "   'description': None,\n",
       "   'ruwiki': 'Каньоны_(фильм)',\n",
       "   'views': 177,\n",
       "   'source': 'fulltext'},\n",
       "  {'qid': 'Q7275',\n",
       "   'label': 'штат',\n",
       "   'score': 11.693494,\n",
       "   'name': 'государство',\n",
       "   'description': 'суверенная территориальная организация',\n",
       "   'ruwiki': 'Государство',\n",
       "   'views': 20380,\n",
       "   'source': 'fulltext'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestions_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can print them in some more human-readable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T02:59:34.116123Z",
     "start_time": "2020-06-14T02:59:34.109808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: В каком американском штате находится Большой Каньон?\n",
      "\n",
      "Большой каньон\n",
      "(каньон, прорезанный рекой Колорадо в плато Колорадо, штат Аризона, США)\n",
      "Link: https://www.wikidata.org/wiki/Q118841\n",
      "\n",
      "Большой каньон\n",
      "Link: https://www.wikidata.org/wiki/Q3848004\n",
      "\n",
      "Большой каньон\n",
      "(фильм 1991 года)\n",
      "Link: https://www.wikidata.org/wiki/Q1542548\n",
      "\n",
      "Большой Каньон\n",
      "Link: https://www.wikidata.org/wiki/Q32508401\n",
      "\n",
      "велосипед\n",
      "(двухколёсное транспортное средство, приводимое в движение посредством педалей)\n",
      "Link: https://www.wikidata.org/wiki/Q11442\n",
      "\n",
      "каньон\n",
      "Link: https://www.wikidata.org/wiki/Q150784\n",
      "\n",
      "Большой каньон Крыма\n",
      "Link: https://www.wikidata.org/wiki/Q16627582\n",
      "\n",
      "Гора Большой Каньон\n",
      "Link: https://www.wikidata.org/wiki/Q31626940\n",
      "\n",
      "США\n",
      "(федеративное государство в Северной Америке)\n",
      "Link: https://www.wikidata.org/wiki/Q30\n",
      "\n",
      "Каньоны\n",
      "Link: https://www.wikidata.org/wiki/Q3014145\n",
      "\n",
      "государство\n",
      "(суверенная территориальная организация)\n",
      "Link: https://www.wikidata.org/wiki/Q7275\n",
      "\n",
      "Query: Аризона\n",
      "\n",
      "Аризона\n",
      "(штат в США)\n",
      "Link: https://www.wikidata.org/wiki/Q816\n",
      "\n",
      "(793) Аризона\n",
      "(астероид)\n",
      "Link: https://www.wikidata.org/wiki/Q157116\n",
      "\n",
      "Аризона Даймондбэкс\n",
      "Link: https://www.wikidata.org/wiki/Q670376\n",
      "\n",
      "Аризона Уайлдкэтс\n",
      "Link: https://www.wikidata.org/wiki/Q4791464\n",
      "\n",
      "Аризона Сноубоул\n",
      "Link: https://www.wikidata.org/wiki/Q4791400\n",
      "\n",
      "Аризона Хотшотс\n",
      "Link: https://www.wikidata.org/wiki/Q55074938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Suggester.pretty_print(suggestions_question))\n",
    "print(Suggester.pretty_print(suggestions_answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kbqa]",
   "language": "python",
   "name": "conda-env-kbqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
